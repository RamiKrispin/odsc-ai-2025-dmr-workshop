{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "239eec86",
   "metadata": {},
   "source": [
    "# Running LLMs Locally with Docker Model Runner and Python\n",
    "\n",
    "\n",
    "\n",
    "One of the great features of Docker Model Runner is its compatibility with the OpenAI API SDKs. This makes it easy to adapt existing code that uses the OpenAI API to work with DMR and interact with locally running LLMs. In this tutorial, we'll focus on the Python SDK, though the same approach applies to other OpenAI SDKs like JavaScript, Java, Go, .NET, and more.\n",
    "\n",
    "DMR runs as a standalone server so that you can connect to it from both containerized environments and regular local Python environments.\n",
    "\n",
    "<figure>\n",
    " <img src=\"assets/dmr_diagram.png\" width=\"100%\" align=\"center\"/></a>\n",
    "</figure>\n",
    "\n",
    "\n",
    "This notebook focuses on running LLMs locally with Python using:\n",
    "- OpenAI API SDK\n",
    "- LangChain (OpenAI client)\n",
    "\n",
    "## The OpenAI API SDK\n",
    "\n",
    "Let's start by loading the `openai` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e18f519c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadd4bae",
   "metadata": {},
   "source": [
    "Next, weâ€™ll define the client using the `OpenAI` function. To connect to the DMR server, we need to set the `base_url` parameter. The value of this URL depends on whether you're running the code inside a container or from your local environment.\n",
    "\n",
    "If you're running the code inside a container, use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "93c70be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url= \"http://model-runner.docker.internal/engines/v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e23e80",
   "metadata": {},
   "source": [
    "If you're running it locally (outside of a container), use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9c67400a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_url = \"http://localhost:12434/engines/v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09c1fc3",
   "metadata": {},
   "source": [
    "> Note that the localhost should point to the TCP port, in this case 12434.\n",
    "\n",
    "Let's initialize the OpenAI client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76b9b50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "  base_url = base_url,\n",
    "  api_key = \"docker\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621251cf",
   "metadata": {},
   "source": [
    "Next, let's set the `chat.completions` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d5ee243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"ai/llama3.2:latest\", \n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a Docker joke\"}\n",
    "        ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d24561",
   "metadata": {},
   "source": [
    "The prompt is send to the local LLM and return the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4ec9c303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Why did Docker go to therapy?\n",
      "\n",
      "Because it was struggling to container-ize its emotions!\n",
      "\n",
      "(Sorry, I know it's a bit of a \"docker\"-ious pun, but I hope it made you smile!)\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eaffce",
   "metadata": {},
   "source": [
    "## Using DMR with LangChain\n",
    "\n",
    "Likewise, we can use the LangChain's OpenAI SDK wrapper to call local LLMs with DRM. Let's start by import the required LangChain methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8c03bba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_core.messages import (\n",
    "  SystemMessage,\n",
    "  HumanMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e0f2c4",
   "metadata": {},
   "source": [
    "Next, we will set the `ChatOpenAI` method with the DMR parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eb2c3c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(    \n",
    "    base_url=\"http://model-runner.docker.internal/engines/v1\",\n",
    "    api_key=\"docker\", \n",
    "    temperature=0, \n",
    "    model = \"ai/llama3.2:latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb067cd",
   "metadata": {},
   "source": [
    "We use the LangChain's built-in prompts functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9fc76463",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\n",
    "  SystemMessage(\n",
    "  \"\"\"\"\n",
    "  You are an AI assistant that helps users with their questions. Your responses should be helpful and informative.\n",
    "  \"\"\"\n",
    "  ),\n",
    "  HumanMessage(\"What is the capital of United States of America?\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7b4014",
   "metadata": {},
   "source": [
    "And send the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "83c08c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82b9799",
   "metadata": {},
   "source": [
    "Let's print the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c9b8e69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of the United States of America is Washington, D.C. (short for District of Columbia).\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-3.11-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
